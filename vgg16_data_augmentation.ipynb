{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "\n",
    "import os, json\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "from scipy import misc, ndimage\n",
    "from scipy.ndimage.interpolation import zoom\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Lambda\n",
    "from keras.layers.convolutional import MaxPooling2D, ZeroPadding2D, Conv2D\n",
    "from keras.layers.pooling import GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD, RMSprop, Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.models import model_from_json\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'data/sample/'\n",
    "path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32)\n",
    "vgg_mean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_preprocess(x):\n",
    "    \"\"\"\n",
    "        Subtracts the mean RGB value, and transposes RGB to BGR.\n",
    "        The mean RGB was computed on the image set used to train the VGG model.\n",
    "        Args: \n",
    "            x: Image array (height x width x channels)\n",
    "        Returns:\n",
    "            Image array (height x width x transposed_channels)\n",
    "    \"\"\"\n",
    "    vgg_mean = np.array([123.68, 116.779, 103.939], dtype=np.float32)\n",
    "    x = x - vgg_mean\n",
    "    return x[:, ::-1] # reverse axis rgb->bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Lambda(vgg_preprocess, input_shape = (224, 224, 3), output_shape = (224, 224, 3)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(128, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(256, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(256, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(256, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(512, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(512, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "\n",
    "model.add(Conv2D(512, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(512, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(Conv2D(512, (3, 3), padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling2D((2, 2), strides = (2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4096, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(4096, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1000, activation = 'softmax'))\n",
    "\n",
    "model.compile(optimizer = Adam(lr = 0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "fname = 'weights/vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "model.load_weights(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.pop()\n",
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(2, activation = 'softmax'))\n",
    "model.compile(optimizer = Adam(lr = 0.001), loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.ImageDataGenerator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "trn_datagen = image.ImageDataGenerator(rotation_range=60., zoom_range=0.30, shear_range=0.20,\n",
    "                                      horizontal_flip=True)\n",
    "trn_batches = trn_datagen.flow_from_directory(path + 'train/', target_size = (224, 224),\n",
    "            class_mode = 'categorical', shuffle = True, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# without Data Augmentation\n",
    "trn_datagen = image.ImageDataGenerator()\n",
    "trn_batches = trn_datagen.flow_from_directory(path + 'train/', target_size = (224, 224),\n",
    "            class_mode = 'categorical', shuffle = True, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2500 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "val_datagen = image.ImageDataGenerator()\n",
    "val_batches = val_datagen.flow_from_directory(path + 'valid/', target_size = (224, 224),\n",
    "            class_mode = 'categorical', shuffle = True, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "433/432 [==============================] - 648s 1s/step - loss: 0.5608 - acc: 0.8135 - val_loss: 0.2722 - val_acc: 0.9056\n",
      "Epoch 2/10\n",
      "433/432 [==============================] - 306s 707ms/step - loss: 0.4735 - acc: 0.8383 - val_loss: 0.2938 - val_acc: 0.8928\n",
      "Epoch 3/10\n",
      "433/432 [==============================] - 372s 859ms/step - loss: 0.4709 - acc: 0.8393 - val_loss: 0.2894 - val_acc: 0.9000\n",
      "Epoch 4/10\n",
      "433/432 [==============================] - 335s 773ms/step - loss: 0.4933 - acc: 0.8350 - val_loss: 0.3543 - val_acc: 0.8840\n",
      "Epoch 5/10\n",
      "433/432 [==============================] - 344s 793ms/step - loss: 0.4647 - acc: 0.8424 - val_loss: 0.2724 - val_acc: 0.9028\n",
      "Epoch 6/10\n",
      "433/432 [==============================] - 324s 749ms/step - loss: 0.4910 - acc: 0.8395 - val_loss: 0.2472 - val_acc: 0.9124\n",
      "Epoch 7/10\n",
      "433/432 [==============================] - 340s 786ms/step - loss: 0.4740 - acc: 0.8401 - val_loss: 0.2480 - val_acc: 0.9132\n",
      "Epoch 8/10\n",
      "433/432 [==============================] - 341s 787ms/step - loss: 0.4717 - acc: 0.8429 - val_loss: 0.2724 - val_acc: 0.8996\n",
      "Epoch 9/10\n",
      "433/432 [==============================] - 324s 747ms/step - loss: 0.4926 - acc: 0.8350 - val_loss: 0.2570 - val_acc: 0.9060\n",
      "Epoch 10/10\n",
      "433/432 [==============================] - 323s 745ms/step - loss: 0.4981 - acc: 0.8364 - val_loss: 0.3330 - val_acc: 0.8940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3435590be0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data Augmentation\n",
    "model.fit_generator(trn_batches, steps_per_epoch = trn_batches.n / 52, epochs = 10, validation_data = val_batches, \n",
    "                    validation_steps = val_batches.n / 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "433/432 [==============================] - 216s 498ms/step - loss: 0.4357 - acc: 0.8720 - val_loss: 0.2627 - val_acc: 0.9008\n",
      "Epoch 2/10\n",
      "433/432 [==============================] - 604s 1s/step - loss: 0.4205 - acc: 0.8742 - val_loss: 0.2337 - val_acc: 0.9136\n",
      "Epoch 3/10\n",
      "433/432 [==============================] - 275s 635ms/step - loss: 0.4161 - acc: 0.8729 - val_loss: 0.2333 - val_acc: 0.9200\n",
      "Epoch 4/10\n",
      "433/432 [==============================] - 456s 1s/step - loss: 0.4183 - acc: 0.8753 - val_loss: 0.2578 - val_acc: 0.9092\n",
      "Epoch 5/10\n",
      "433/432 [==============================] - 343s 792ms/step - loss: 0.4329 - acc: 0.8729 - val_loss: 0.2617 - val_acc: 0.9136\n",
      "Epoch 6/10\n",
      "433/432 [==============================] - 222s 513ms/step - loss: 0.4300 - acc: 0.8754 - val_loss: 0.2055 - val_acc: 0.9228\n",
      "Epoch 7/10\n",
      "433/432 [==============================] - 218s 504ms/step - loss: 0.4155 - acc: 0.8771 - val_loss: 0.2431 - val_acc: 0.9148\n",
      "Epoch 8/10\n",
      "433/432 [==============================] - 219s 506ms/step - loss: 0.4410 - acc: 0.8722 - val_loss: 0.2199 - val_acc: 0.9168\n",
      "Epoch 9/10\n",
      "433/432 [==============================] - 219s 505ms/step - loss: 0.4186 - acc: 0.8779 - val_loss: 0.2505 - val_acc: 0.9120\n",
      "Epoch 10/10\n",
      "433/432 [==============================] - 256s 590ms/step - loss: 0.4159 - acc: 0.8794 - val_loss: 0.2564 - val_acc: 0.9084\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3433765320>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without Data Augmentation\n",
    "model.fit_generator(trn_batches, steps_per_epoch = trn_batches.n / 52, epochs = 10, validation_data = val_batches, \n",
    "                    validation_steps = val_batches.n / 52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
